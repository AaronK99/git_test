{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6CwQSOnLMW1Gbo7+5THsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronK99/git_test/blob/main/Ensemble_Methods_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 1\n",
        "\n",
        "Construct the following models on the same dataset:\n",
        "- Bagging\n",
        "- Random Forest\n",
        "- Adaboost\n",
        "\n",
        "Compare their performance and write a short paragraph on which one is the best. You are free to change the hyperparameters.\n"
      ],
      "metadata": {
        "id": "LJRDNBrIa9_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Construct and train Bagging model\n",
        "bagging_model = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Construct and train Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Construct and train AdaBoost model\n",
        "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "adaboost_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "adaboost_acc = accuracy_score(y_test, adaboost_pred)\n",
        "\n",
        "# Display the results with more precision\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Bagging', 'Random Forest', 'AdaBoost'],\n",
        "    'Accuracy': [bagging_acc, rf_acc, adaboost_acc]\n",
        "})\n",
        "\n",
        "print(\"Accuracy Scores:\")\n",
        "print(results)\n",
        "\n",
        "print(\"\\nClassification Reports:\")\n",
        "print(\"Bagging Classifier:\")\n",
        "print(classification_report(y_test, bagging_pred, digits=4))\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(classification_report(y_test, rf_pred, digits=4))\n",
        "print(\"AdaBoost Classifier:\")\n",
        "print(classification_report(y_test, adaboost_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJlaLzCwYJoA",
        "outputId": "0e0f17ff-0530-4366-ce0c-d6a5c2f59401"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Scores:\n",
            "           Model  Accuracy\n",
            "0        Bagging  0.959064\n",
            "1  Random Forest  0.970760\n",
            "2       AdaBoost  0.976608\n",
            "\n",
            "Classification Reports:\n",
            "Bagging Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9516    0.9365    0.9440        63\n",
            "           1     0.9633    0.9722    0.9677       108\n",
            "\n",
            "    accuracy                         0.9591       171\n",
            "   macro avg     0.9575    0.9544    0.9559       171\n",
            "weighted avg     0.9590    0.9591    0.9590       171\n",
            "\n",
            "Random Forest Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9833    0.9365    0.9593        63\n",
            "           1     0.9640    0.9907    0.9772       108\n",
            "\n",
            "    accuracy                         0.9708       171\n",
            "   macro avg     0.9736    0.9636    0.9683       171\n",
            "weighted avg     0.9711    0.9708    0.9706       171\n",
            "\n",
            "AdaBoost Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9683    0.9683    0.9683        63\n",
            "           1     0.9815    0.9815    0.9815       108\n",
            "\n",
            "    accuracy                         0.9766       171\n",
            "   macro avg     0.9749    0.9749    0.9749       171\n",
            "weighted avg     0.9766    0.9766    0.9766       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AdaBoost classifier performed the best on the Breast Cancer dataset with an accuracy of 97.66%, followed closely by the Random Forest classifier at 97.08%. The Bagging classifier had the lowest accuracy at 95.91%. Overall, AdaBoost's ability to handle misclassified instances adaptively gave it a slight edge over the others. While all three models performed well, AdaBoost is the best choice for this dataset."
      ],
      "metadata": {
        "id": "Q-PAKYGoZQsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "The accuracy for this dataset is quite low. Can you try any other method that increases the accuracy. You can try either Random Forest or Adaboost. What do you notice?"
      ],
      "metadata": {
        "id": "orFMZIoublHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "cuisines_df = pd.read_csv(\"https://an-utd-python.s3.us-west-1.amazonaws.com/cuisines.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "cuisines_label_df = cuisines_df['cuisine']\n",
        "cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "gb_grid_search = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=gb_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "gb_grid_search.fit(X_train_scaled, y_train)\n",
        "best_gb = gb_grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best Gradient Boosting model\n",
        "y_pred_gb = best_gb.predict(X_test_scaled)\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Best Gradient Boosting Model Accuracy:\", accuracy_gb)\n",
        "print(classification_report(y_test, y_pred_gb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5cwPquZorp",
        "outputId": "f8a2c938-841c-40c2-e3cc-78dec837b27f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best Gradient Boosting Model Accuracy: 0.8165137614678899\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     chinese       0.76      0.73      0.75       236\n",
            "      indian       0.90      0.94      0.92       245\n",
            "    japanese       0.78      0.78      0.78       231\n",
            "      korean       0.83      0.77      0.80       242\n",
            "        thai       0.80      0.86      0.83       245\n",
            "\n",
            "    accuracy                           0.82      1199\n",
            "   macro avg       0.82      0.82      0.81      1199\n",
            "weighted avg       0.82      0.82      0.82      1199\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimized Random Forest model performed the best on the cuisines dataset with an accuracy of 84.07%, making it the most effective choice. The Gradient Boosting model also showed good performance with an accuracy of 81.65%, but it did not surpass Random Forest. AdaBoost had the lowest accuracy at 67.14%, indicating it is less suitable for this dataset. Overall, Random Forest is recommended due to its highest accuracy and balanced performance across different cuisine classes. Further hyperparameter tuning or exploring other models could potentially enhance results even more.\n"
      ],
      "metadata": {
        "id": "KJk7zGfLfrPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "Try other combination of hyperparameters for Random Forest and AdaBoost models and check how good of an accuracy you can obtain."
      ],
      "metadata": {
        "id": "bntboNH9cF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "cuisines_df = pd.read_csv(\"https://an-utd-python.s3.us-west-1.amazonaws.com/cuisines.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "cuisines_label_df = cuisines_df['cuisine']\n",
        "cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define pipelines with standard scaling\n",
        "rf_pipe = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=42))])\n",
        "ab_pipe = Pipeline([('scaler', StandardScaler()), ('ab', AdaBoostClassifier(random_state=42))])\n",
        "\n",
        "# Define extended hyperparameter grids\n",
        "rf_param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 300, 500],\n",
        "    'rf__max_depth': [7, 9, 11, 13],\n",
        "    'rf__max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'rf__min_samples_split': [2, 5, 10],\n",
        "    'rf__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "ab_param_grid = {\n",
        "    'ab__n_estimators': [50, 100, 200, 300],\n",
        "    'ab__learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Perform grid search for Random Forest\n",
        "rf_grid_search = GridSearchCV(estimator=rf_pipe, param_grid=rf_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "best_rf = rf_grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best Random Forest model\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Best Random Forest Model Accuracy:\", accuracy_rf)\n",
        "print(\"Best Random Forest Parameters:\", rf_grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Perform grid search for AdaBoost\n",
        "ab_grid_search = GridSearchCV(estimator=ab_pipe, param_grid=ab_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "ab_grid_search.fit(X_train, y_train)\n",
        "best_ab = ab_grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best AdaBoost model\n",
        "y_pred_ab = best_ab.predict(X_test)\n",
        "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
        "print(\"Best AdaBoost Model Accuracy:\", accuracy_ab)\n",
        "print(\"Best AdaBoost Parameters:\", ab_grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred_ab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giIb3HSyf990",
        "outputId": "a7b6ffe2-2a31-4ada-fad2-727d9f2d7b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
          ]
        }
      ]
    }
  ]
}